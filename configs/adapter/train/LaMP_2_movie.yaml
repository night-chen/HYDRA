task: LaMP_2_movie
mode: train
use_profile: True
train_dir: data/LaMP-RAG-k_4/LaMP_2_movie/train.json
eval_history: data/LaMP-RAG-k_4/LaMP_2_movie/dev_history.json
eval_query: data/LaMP-RAG-k_4/LaMP_2_movie/test_query.jsonl
partial_data:
  users: 100
retriever:
  num_retrieve: 1
  model: bm25
  is_ranked: True
  max_length: 1024
  tokenizer: None
generator:
  model_name: gpt
  tokenizer_name: gpt
  max_length: 512
  data_frac: 0
  frac_len: 0
  output_dir: data/LaMP_2_movie/generation/openai/p-rag/
  batch_size: 8
  token: hf_vpwLytvCDSCgTJryTUagradeoXDSzoUsJ
  seed: 40
  temperature: 1
  tp_per_worker: 1
  num_data_frac: 1
  num_return_sequences: 8
  frequency_penalty: 0 
  presence_penalty: 0 
  stop: None
  openai_credentials: chao-3.5
trainer:
  model_name:  allenai/longformer-base-4096
  tokenizer_name: allenai/longformer-base-4096
  output_dir: checkpoints/p/LongFormer-cls-p-LaMP_2_movie
  save_dir: checkpoints/p/LongFormer-cls-p-LaMP_2_movie-user
  learning_rate: 2.0e-5
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 32
  num_train_epochs: 2
  weight_decay: 0.01
  save_strategy: epoch
  push_to_hub: False
  gradient_accumulation_steps: 1
  l2_reg_coef: 1.0
  energy_temp: 5.0
  add_special_tokens: False