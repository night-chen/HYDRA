task: LaMP_2_movie
mode: train
use_profile: True
train_dir: <TRAIN_DATA_PATH>
eval_history: <TEST_HISTORY_DATA_PATH>
eval_query: <TEST_QUERY_DATA_PATH>
partial_data:
  users: 100
retriever:
  num_retrieve: 10
  model: bm25
  is_ranked: True
  max_length: 1024
  tokenizer: None
generator:
  model_name: gpt
  tokenizer_name: gpt
  max_length: 64
  data_frac: 0
  frac_len: 0
  output_dir: <OUTPUT_PATH>
  batch_size: 8
  token: <HUGGINGFACE_TOKEN>
  seed: 40
  temperature: 0
  tp_per_worker: 1
  num_data_frac: 1
  num_return_sequences: 1
  frequency_penalty: 0 
  presence_penalty: 0 
  stop: None
  openai_credentials: chao-3.5
trainer:
  model_name:  <OUTPUT_MODEL_NAME>
  tokenizer_name: allenai/longformer-base-4096
  output_dir: <OUTPUT_MODEL_PATH>
  save_dir: <SAME_PATH>
  learning_rate: 2.0e-5
  per_device_train_batch_size: 64
  per_device_eval_batch_size: 512
  num_train_epochs: 2
  weight_decay: 0.01
  save_strategy: epoch
  push_to_hub: False
  gradient_accumulation_steps: 1
  l2_reg_coef: 1.0
  energy_temp: 5.0
  add_special_tokens: False